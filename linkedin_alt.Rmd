---
title: "tidy and clean linkedin scraped data"
author: "T. Jenkins"
date: "10/19/2019"
output: 
  html_document:
    toc: true
---

# Introduction

As part of our project, we are tasked to answer the question "What are the most valued data science skills?" by working as a team, deciding what data to collect and how to collect it, use relational database and set of normalized tables and data exploration and analysis. Our team members are as follows;

- Anil Akyildirim

- Nicholas Chung

- Jai Jeffryes

- Tamiko Jenkins

- Joe Rovalino

- Sie Siong Wong

As part of project management tools, we have used Slack Private channel and Skype for Project Communication, Github for Project tracking, documentation and code collaboration, and Amazon Relational Database Service for data integration. All of our supporting code and data are on the GitHub repo, which documents branches and commits from our team.

- GitHub: [https://github.com/pnojai/dskill](https://github.com/pnojai/dskill)
- Amazon Relational Database Service: [msds607.ckxhi71v1dqf.us-east-1.rds.amazonaws.com](msds607.ckxhi71v1dqf.us-east-1.rds.amazonaws.com)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load packages
#library(rjson)
#library(tidyr)
library(dplyr)
library(jsonlite)
#library(purrr)
#library(janitor)
library(ggplot2)
#library(reshape2)
#library(plyr)
#library(zoo)
library(RMariaDB)
```

# Data Collection 

We have reviewed and discussed different data types such as current job requirements around data scientists from job postings such as indeed.com or monster.com and articles around top data scientists skills in websites such as towardsdatascience and knuggets. Our approach built on the assumption that data scientists with jobs have the skills most valued by employers. We collected skills from employed data scientists.

we were inspired by the research of Jeff Hale whose article on data science skills appeared on the website, Medium.

- [https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db](https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db).

We discussed different methods of collecting the data and further how we can store it. As a result, we decided to work with usefull data within linkedin.com. We compared our findings from LinkedIn data to Mr. Hale's 2018 findings.


### Load JSON files
```{r}
# load all JSON
filenames <- list.files("data/profiles", pattern="*.json", full.names=TRUE) # this should give you a character vector, with each file name represented by an entry
example_file <- lapply(filenames[1], function(x) jsonlite::fromJSON(txt = x)) # a list in which each element is one of your original JSON files
example_file
```


### Bind fromJSON results
```{r}
# apply fromJSON to read in all of the json files
# create the column (variable) title, headline, which will be populated with json file identifying information
# extract the skills data which contains the variables title and counts
# bind the results together as a data frame named r_df
r_df <- dplyr::bind_rows(sapply(filenames, function(x) fromJSON(x, flatten=TRUE)$skills), .id="headline")

```


### Extract Headlines
```{r}
# apply fromJSON to read in all of the json files
# extract the headline variable from the profile data, saving each file name as the variable title
# save the mapping as  data frame headlines 
headlines <- sapply(filenames, function(x) fromJSON(x, flatten=TRUE)$profile$headline, USE.NAMES = TRUE)
```


### Map Headlines to Skills
```{r}
#  apply a look up of the variable title specifying the filename 
#  and add the headline value from the headlines  data frame
#  to the headlines variable in  data frame r_df

r_df$headline <- sapply(r_df$headline, function(x) headlines[x])

```


### Name and convert variables and data
```{r}
df <- r_df
# TODO: conform names to db
# TODO: Fix naming call
# TODO: find out which naming convention we're using
# display data frame r_df
head(df)
# name r_df
names(df) <- c("title", "skills", "count")
#names(v) <- c("headline", "skills", "linkedin")
class(df)
# create numeric values in Linkedin counts column
# coerce any nulls to na's
sapply(df, class)
df$count <- as.numeric(df$count)
sapply(df, class)
```


### Remove NA's from numeric column
```{r}
# count all rows
# 4822
# view a subset of rows with na's mixed with complete rows
nrow(df)
df[11:20,]

# filter for any rows with na
# count all rows with na's
# 942
# view a subset of rows with only na's
df_na <- df %>% filter_all(any_vars(is.na(.)))
nrow(df_na)
df[1:20,]

# omit any rows with na's
# save rows without na's as a data frame names df
# count the data frame 
# 4822-942 = 3880
df <- na.omit(df)
df
nrow(df)
```


### Simplify data frame

```{r}

# TODO: simplify
# subset vector v, removing the headline variable
# save the new vector as v_counts
df_counts <- df[c(-1)]

```


### View aggregate Skills counts
```{r}

# aggregate the counts for each unique skill
# store as agg_df_counts data fram
agg_df_counts <- df_counts %>%
    group_by(skills)  %>%
    dplyr::summarise(count = n())  %>%
    arrange(desc(count))

agg_df_counts
```

# Prepare data for storage
```{r}

# TODO: follow df.csv convention

# Add rownames (indices) as a skill id
# to final dataframe to prepare for 
# SQL-based storage and to provide option to
# remove automatic row names from write csv
# Remove depr function
#df_csv <- add_rownames(df, var = "skill_id")
df_csv <- tibble::rownames_to_column(df, var = "skill_id")
head(df_csv)

# TODO: follow df.csv convention
# Rearrange column order with dplyr select
df_csv <- dplyr::select(df_csv, skill_id, skills, count, title)

head(df_csv)

```

## Write to csv
```{r}
# write csv and upload to our mysql database
write.csv(df_csv, "results/df_alt.csv", row.names=FALSE)

```
